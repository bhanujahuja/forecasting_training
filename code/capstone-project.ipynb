{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44d5397",
   "metadata": {},
   "source": [
    "# Capstone Project: End-to-End Time Series Forecasting\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this capstone project, you will build a complete forecasting solution from scratch, integrating methods from all previous modules (baselines, statistical, ML, and deep learning).\n",
    "\n",
    "## Project Structure (7 Phases, ~12-15 hours total)\n",
    "\n",
    "### Phase 1: Problem Definition & EDA (2-3 hours)\n",
    "- Select or load a time series dataset\n",
    "- Exploratory data analysis\n",
    "- Identify seasonality, trends, outliers\n",
    "- Decompose the series\n",
    "- Statistical tests for stationarity\n",
    "\n",
    "### Phase 2: Baseline Models (2 hours)\n",
    "- Implement naive, seasonal naive, moving average\n",
    "- Calculate baseline metrics\n",
    "- Establish performance floor\n",
    "\n",
    "### Phase 3: Statistical Models (2-3 hours)\n",
    "- ARIMA/SARIMA selection and fitting\n",
    "- Auto ARIMA\n",
    "- Prophet implementation\n",
    "- Compare with baselines\n",
    "\n",
    "### Phase 4: Machine Learning (2-3 hours)\n",
    "- Feature engineering\n",
    "- Random Forest, XGBoost, LightGBM\n",
    "- Hyperparameter tuning\n",
    "- Feature importance analysis\n",
    "\n",
    "### Phase 5: Deep Learning (2-3 hours)\n",
    "- LSTM, CNN, Hybrid architectures\n",
    "- Training with callbacks\n",
    "- Cross-architecture comparison\n",
    "\n",
    "### Phase 6: Model Ensemble (1-2 hours)\n",
    "- Combine predictions from best models\n",
    "- Weighted ensemble\n",
    "- Stacking\n",
    "\n",
    "### Phase 7: Final Evaluation & Documentation (1-2 hours)\n",
    "- Best model selection\n",
    "- Deployment considerations\n",
    "- Documentation and reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ebb5e",
   "metadata": {},
   "source": [
    "# Capstone Project: End-to-End Forecasting Pipeline\n",
    "\n",
    "## Project Title: [Your Forecasting Problem]\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Date:** [Date]  \n",
    "**Duration:** [Total Hours]\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "[2-3 paragraph summary]\n",
    "- What problem are you solving?\n",
    "- What methodology did you use?\n",
    "- What are the key findings?\n",
    "- What recommendations do you make?\n",
    "- What is the expected business impact?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b5fe6f",
   "metadata": {},
   "source": [
    "## Phase 1: Problem Definition & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0adcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load airline dataset (or use your own)\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df['Time'] = pd.date_range(start='1949-01', periods=len(df), freq='MS')\n",
    "df = df.set_index('Time')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nDataset Overview:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Date Range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "print(df['Passengers'].describe())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "axes[0, 0].plot(df.index, df['Passengers'], linewidth=2)\n",
    "axes[0, 0].set_title('Time Series', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Passengers')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(df['Passengers'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Distribution', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Passengers')\n",
    "\n",
    "axes[1, 0].plot(df['Passengers'].diff(), linewidth=1)\n",
    "axes[1, 0].set_title('First Differences', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Δ Passengers')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# ACF-like visualization\n",
    "rolling_mean = df['Passengers'].rolling(window=12).mean()\n",
    "rolling_std = df['Passengers'].rolling(window=12).std()\n",
    "axes[1, 1].plot(df.index, df['Passengers'], alpha=0.5, label='Original')\n",
    "axes[1, 1].plot(rolling_mean.index, rolling_mean, label='12-Month Rolling Mean', linewidth=2)\n",
    "axes[1, 1].fill_between(rolling_std.index, rolling_mean - rolling_std, rolling_mean + rolling_std, alpha=0.2)\n",
    "axes[1, 1].set_title('Trend & Seasonality', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  ✓ Clear upward trend from 1949-1960\")\n",
    "print(\"  ✓ Strong 12-month seasonality\")\n",
    "print(\"  ✓ No missing values\")\n",
    "print(\"  ✓ Data appears to be non-stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae651a",
   "metadata": {},
   "source": [
    "## Phase 2: Baseline Models\n",
    "\n",
    "Quick implementations of naive, seasonal naive, and moving average forecasts to establish a performance floor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8dc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Baseline Models\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 2: BASELINE MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train/test split\n",
    "train_data = df['Passengers'][:-12]\n",
    "test_data = df['Passengers'][-12:]\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "# 1. Naive\n",
    "naive_forecast = pd.Series([train_data.iloc[-1]] * len(test_data), index=test_data.index)\n",
    "baseline_results['Naive'] = {\n",
    "    'forecast': naive_forecast.values,\n",
    "    'mae': mean_absolute_error(test_data, naive_forecast),\n",
    "    'rmse': np.sqrt(mean_squared_error(test_data, naive_forecast))\n",
    "}\n",
    "\n",
    "# 2. Seasonal Naive\n",
    "seasonal_naive = pd.Series(train_data.iloc[-12:].values, index=test_data.index)\n",
    "baseline_results['Seasonal Naive'] = {\n",
    "    'forecast': seasonal_naive.values,\n",
    "    'mae': mean_absolute_error(test_data, seasonal_naive),\n",
    "    'rmse': np.sqrt(mean_squared_error(test_data, seasonal_naive))\n",
    "}\n",
    "\n",
    "# 3. Moving Average (12-month)\n",
    "ma_value = train_data.tail(12).mean()\n",
    "ma_forecast = pd.Series([ma_value] * len(test_data), index=test_data.index)\n",
    "baseline_results['MA(12)'] = {\n",
    "    'forecast': ma_forecast.values,\n",
    "    'mae': mean_absolute_error(test_data, ma_forecast),\n",
    "    'rmse': np.sqrt(mean_squared_error(test_data, ma_forecast))\n",
    "}\n",
    "\n",
    "print(\"\\nBaseline Model Performance:\")\n",
    "for model_name, results in baseline_results.items():\n",
    "    print(f\"  {model_name}:\")\n",
    "    print(f\"    MAE:  {results['mae']:.2f}\")\n",
    "    print(f\"    RMSE: {results['rmse']:.2f}\")\n",
    "\n",
    "# Store for later comparison\n",
    "all_models = baseline_results.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c22073",
   "metadata": {},
   "source": [
    "## Phase 3: Statistical Methods (ARIMA, Prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Statistical Models\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 3: STATISTICAL METHODS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "\n",
    "# SARIMA\n",
    "print(\"\\n1. SARIMA with Auto ARIMA...\")\n",
    "auto_model = auto_arima(train_data, seasonal=True, m=12, trace=False, stepwise=True)\n",
    "sarima_forecast = auto_model.predict(n_periods=len(test_data))\n",
    "\n",
    "all_models['SARIMA'] = {\n",
    "    'forecast': sarima_forecast.values,\n",
    "    'mae': mean_absolute_error(test_data, sarima_forecast),\n",
    "    'rmse': np.sqrt(mean_squared_error(test_data, sarima_forecast))\n",
    "}\n",
    "\n",
    "print(f\"   SARIMA{auto_model.order}{auto_model.seasonal_order}\")\n",
    "print(f\"   RMSE: {all_models['SARIMA']['rmse']:.2f}\")\n",
    "\n",
    "# Prophet\n",
    "print(\"\\n2. Prophet...\")\n",
    "prophet_train = pd.DataFrame({'ds': train_data.index, 'y': train_data.values})\n",
    "prophet_model = Prophet(yearly_seasonality=True, daily_seasonality=False)\n",
    "prophet_model.fit(prophet_train)\n",
    "\n",
    "future = prophet_model.make_future_dataframe(periods=len(test_data), freq='MS')\n",
    "prophet_fcst = prophet_model.predict(future)\n",
    "prophet_values = prophet_fcst['yhat'].iloc[-len(test_data):].values\n",
    "\n",
    "all_models['Prophet'] = {\n",
    "    'forecast': prophet_values,\n",
    "    'mae': mean_absolute_error(test_data, prophet_values),\n",
    "    'rmse': np.sqrt(mean_squared_error(test_data, prophet_values))\n",
    "}\n",
    "\n",
    "print(f\"   RMSE: {all_models['Prophet']['rmse']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f22b296",
   "metadata": {},
   "source": [
    "## Phase 4-5: ML and Deep Learning Models\n",
    "\n",
    "Quick implementations of Random Forest and LSTM for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4-5: ML and DL\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 4-5: MACHINE LEARNING & DEEP LEARNING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Feature engineering for ML\n",
    "def create_ml_features(data):\n",
    "    df = pd.DataFrame({'y': data})\n",
    "    for lag in [1, 3, 6, 12]:\n",
    "        df[f'lag_{lag}'] = df['y'].shift(lag)\n",
    "    df['ma_12'] = df['y'].rolling(12).mean()\n",
    "    return df.dropna()\n",
    "\n",
    "ml_data = create_ml_features(train_data)\n",
    "X_train_ml = ml_data.drop('y', axis=1)\n",
    "y_train_ml = ml_data['y']\n",
    "\n",
    "# Test set features\n",
    "test_ml_data = create_ml_features(pd.concat([train_data, test_data]))\n",
    "X_test_ml = test_ml_data.iloc[-len(test_data):].drop('y', axis=1)\n",
    "y_test_ml = test_ml_data.iloc[-len(test_data):]['y']\n",
    "\n",
    "# Random Forest\n",
    "print(\"\\n1. Random Forest...\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_ml, y_train_ml)\n",
    "rf_pred = rf_model.predict(X_test_ml)\n",
    "\n",
    "all_models['Random Forest'] = {\n",
    "    'forecast': rf_pred,\n",
    "    'mae': mean_absolute_error(y_test_ml, rf_pred),\n",
    "    'rmse': np.sqrt(mean_squared_error(y_test_ml, rf_pred))\n",
    "}\n",
    "print(f\"   RMSE: {all_models['Random Forest']['rmse']:.2f}\")\n",
    "\n",
    "# XGBoost\n",
    "print(\"\\n2. XGBoost...\")\n",
    "xgb_model = XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, verbosity=0)\n",
    "xgb_model.fit(X_train_ml, y_train_ml)\n",
    "xgb_pred = xgb_model.predict(X_test_ml)\n",
    "\n",
    "all_models['XGBoost'] = {\n",
    "    'forecast': xgb_pred,\n",
    "    'mae': mean_absolute_error(y_test_ml, xgb_pred),\n",
    "    'rmse': np.sqrt(mean_squared_error(y_test_ml, xgb_pred))\n",
    "}\n",
    "print(f\"   RMSE: {all_models['XGBoost']['rmse']:.2f}\")\n",
    "\n",
    "# LSTM\n",
    "print(\"\\n3. LSTM...\")\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df['Passengers'].values.reshape(-1, 1))\n",
    "\n",
    "def create_sequences(data, seq_len=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        X.append(data[i:i+seq_len])\n",
    "        y.append(data[i+seq_len, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_lstm, y_lstm = create_sequences(scaled_data, 12)\n",
    "split_idx = int(0.8 * len(X_lstm))\n",
    "X_train_lstm, X_test_lstm = X_lstm[:split_idx], X_lstm[split_idx:]\n",
    "y_train_lstm, y_test_lstm = y_lstm[:split_idx], y_lstm[split_idx:]\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, input_shape=(12, 1)),\n",
    "    Dropout(0.2),\n",
    "    Dense(25),\n",
    "    Dense(1)\n",
    "])\n",
    "lstm_model.compile(optimizer=Adam(0.001), loss='mse')\n",
    "lstm_model.fit(X_train_lstm, y_train_lstm, epochs=50, batch_size=16, validation_split=0.2,\n",
    "               callbacks=[EarlyStopping(patience=10)], verbose=0)\n",
    "\n",
    "lstm_pred_scaled = lstm_model.predict(X_test_lstm, verbose=0)\n",
    "lstm_pred = scaler.inverse_transform(lstm_pred_scaled)\n",
    "y_test_lstm_original = scaler.inverse_transform(y_test_lstm.reshape(-1, 1))\n",
    "\n",
    "all_models['LSTM'] = {\n",
    "    'forecast': lstm_pred.flatten()[:len(test_data)],\n",
    "    'mae': mean_absolute_error(y_test_lstm_original, lstm_pred),\n",
    "    'rmse': np.sqrt(mean_squared_error(y_test_lstm_original, lstm_pred))\n",
    "}\n",
    "print(f\"   RMSE: {all_models['LSTM']['rmse']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385849e3",
   "metadata": {},
   "source": [
    "## Phase 6-7: Final Evaluation and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Comparison\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 6-7: FINAL MODEL COMPARISON AND RECOMMENDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_summary = []\n",
    "for model_name, results in all_models.items():\n",
    "    results_summary.append({\n",
    "        'Model': model_name,\n",
    "        'MAE': results['mae'],\n",
    "        'RMSE': results['rmse']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_summary).sort_values('RMSE')\n",
    "print(f\"\\n{results_df.to_string(index=False)}\")\n",
    "\n",
    "best_model = results_df.iloc[0]\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"BEST MODEL: {best_model['Model']}\")\n",
    "print(f\"  RMSE: {best_model['RMSE']:.2f}\")\n",
    "print(f\"=\" * 80)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE Comparison\n",
    "ax = axes[0]\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(results_df)))\n",
    "ax.barh(results_df['Model'], results_df['RMSE'], color=colors)\n",
    "ax.set_xlabel('RMSE (Lower is Better)')\n",
    "ax.set_title('Model Comparison - RMSE', fontweight='bold', fontsize=12)\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "for i, v in enumerate(results_df['RMSE']):\n",
    "    ax.text(v + 0.5, i, f'{v:.2f}', va='center')\n",
    "\n",
    "# Actual vs Best Model\n",
    "ax = axes[1]\n",
    "best_model_name = best_model['Model']\n",
    "best_forecast = all_models[best_model_name]['forecast'][:len(test_data)]\n",
    "ax.plot(test_data.values, 'o-', label='Actual', linewidth=2.5, markersize=7, color='black')\n",
    "ax.plot(best_forecast, 's--', label=f'{best_model_name} Forecast', linewidth=2, markersize=6, color='red')\n",
    "ax.set_xlabel('Test Index')\n",
    "ax.set_ylabel('Passengers')\n",
    "ax.set_title(f'Best Model: {best_model_name}', fontweight='bold', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nRECOMMENDATIONS:\")\n",
    "print(f\"  1. Deploy: {best_model['Model']}\")\n",
    "print(f\"  2. Expected MAE: {best_model['MAE']:.2f} passengers\")\n",
    "print(f\"  3. Expected RMSE: {best_model['RMSE']:.2f} passengers\")\n",
    "print(f\"  4. Next: Monitor performance in production\")\n",
    "print(f\"  5. Re-train monthly with new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd3107",
   "metadata": {},
   "source": [
    "### 2.2 Stationarity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25727147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f368f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed2036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e396ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ecbf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4ba49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2d6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0abe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532e6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195929a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a8aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5b23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaef118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4eb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5011948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302b45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca657ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d374cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66dd3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
